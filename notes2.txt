Investigating the effect of batch size on the audio unet results

see the wandb experiments once they're done
10k steps for baseline
other variants are # of audio sample seen matched
audio seg length 2x -> 1/2 steps
batch size 2x -> 1/2 steps

this way the results are wall clock time matched

giant command to run all experiments:

python train_audio_unet_model.py baseline_
python train_audio_unet_model.py --accumulate_n 2 --step 2 batch2x_
python train_audio_unet_model.py --accumulate_n 4 --step 4 batch4x_
python train_audio_unet_model.py --accumulate_n 8 --step 8 batch8x_

results:
very similar, though batch 8x pulls ahead of baseline at the end in si-sdr
the other 2 do not improve over baseline si-sdr wise
however, for validation loss ema the order is (best to worst): baseline, batch2x, batch4x, batch8x
for validation loss, the order is (best to worst): batch4x, batch2x, batch8x, baseline

to understand better, run the same experiment for 2.5x the training steps

same experiment but with 25k steps:

python train_audio_unet_model.py baseline_25k_ --n_train_steps 25000
python train_audio_unet_model.py --accumulate_n 2 --step 2 batch2x_25k_ --n_train_steps 25000
python train_audio_unet_model.py --accumulate_n 4 --step 4 batch4x_25k_ --n_train_steps 25000
python train_audio_unet_model.py --accumulate_n 8 --step 8 batch8x_25k_ --n_train_steps 25000

50k steps:

python train_audio_unet_model.py baseline_50k_ --n_train_steps 50000
python train_audio_unet_model.py --accumulate_n 2 --step 2 batch2x_50k_ --n_train_steps 50000
python train_audio_unet_model.py --accumulate_n 4 --step 4 batch4x_50k_ --n_train_steps 50000
python train_audio_unet_model.py --accumulate_n 8 --step 8 batch8x_50k_ --n_train_steps 50000

The experiment, but with hybrid model using last 1088 ffts for the spec unet
python train_hybrid_model.py baseline_
python train_hybrid_model.py --accumulate_n 2 --step 2 batch2x_
python train_hybrid_model.py --accumulate_n 4 --step 4 batch4x_
python train_hybrid_model.py --accumulate_n 8 --step 8 batch8x_

The experiment, same as last, but using cosine decay learning rate

python train_hybrid_model.py baseline_cosine_
python train_hybrid_model.py --accumulate_n 2 --step 2 batch2x_cosine_
python train_hybrid_model.py --accumulate_n 4 --step 4 batch4x_cosine_
python train_hybrid_model.py --accumulate_n 8 --step 8 batch8x_cosine_

oops, cosine decay in pytorch actually goes up and down, not just down

using exponential LR decay instead, tuning lr as well

python train_hybrid_model.py baseline_explr_
python train_hybrid_model.py 10x_lr_explr_ --learning_rate 3e-3
python train_hybrid_model.py 0.1x_lr_explr_ --learning_rate 3e-5
python train_hybrid_model.py 3x_lr_explr_ --learning_rate 1.2e-3
python train_hybrid_model.py 0.33x_lr_explr_ --learning_rate 1e-4
