Investigating the effect of batch size on the audio unet results

see the wandb experiments once they're done
10k steps for baseline
other variants are # of audio sample seen matched
audio seg length 2x -> 1/2 steps
batch size 2x -> 1/2 steps

this way the results are wall clock time matched

giant command to run all experiments:

python train_audio_unet_model.py baseline_
python train_audio_unet_model.py --accumulate_n 2 --step 2 batch2x_
python train_audio_unet_model.py --accumulate_n 4 --step 4 batch4x_
python train_audio_unet_model.py --accumulate_n 8 --step 8 batch8x_

results:
very similar, though batch 8x pulls ahead of baseline at the end in si-sdr
the other 2 do not improve over baseline si-sdr wise
however, for validation loss ema the order is (best to worst): baseline, batch2x, batch4x, batch8x
for validation loss, the order is (best to worst): batch4x, batch2x, batch8x, baseline

to understand better, run the same experiment for 2.5x the training steps

same experiment but with 25k steps:

python train_audio_unet_model.py baseline_25k_ --n_train_steps 25000
python train_audio_unet_model.py --accumulate_n 2 --step 2 batch2x_25k_ --n_train_steps 25000
python train_audio_unet_model.py --accumulate_n 4 --step 4 batch4x_25k_ --n_train_steps 25000
python train_audio_unet_model.py --accumulate_n 8 --step 8 batch8x_25k_ --n_train_steps 25000

50k steps:

python train_audio_unet_model.py baseline_50k_ --n_train_steps 50000
python train_audio_unet_model.py --accumulate_n 2 --step 2 batch2x_50k_ --n_train_steps 50000
python train_audio_unet_model.py --accumulate_n 4 --step 4 batch4x_50k_ --n_train_steps 50000
python train_audio_unet_model.py --accumulate_n 8 --step 8 batch8x_50k_ --n_train_steps 50000

The experiment, but with hybrid model using last 1088 ffts for the spec unet
python train_hybrid_model.py baseline_
python train_hybrid_model.py --accumulate_n 2 --step 2 batch2x_
python train_hybrid_model.py --accumulate_n 4 --step 4 batch4x_
python train_hybrid_model.py --accumulate_n 8 --step 8 batch8x_

The experiment, same as last, but using cosine decay learning rate

python train_hybrid_model.py baseline_cosine_
python train_hybrid_model.py --accumulate_n 2 --step 2 batch2x_cosine_
python train_hybrid_model.py --accumulate_n 4 --step 4 batch4x_cosine_
python train_hybrid_model.py --accumulate_n 8 --step 8 batch8x_cosine_

oops, cosine decay in pytorch actually goes up and down, not just down

using exponential LR decay instead, tuning lr as well

python train_hybrid_model.py baseline_explr_
python train_hybrid_model.py 10x_lr_explr_ --learning_rate 3e-3
python train_hybrid_model.py 0.1x_lr_explr_ --learning_rate 3e-5
python train_hybrid_model.py 3x_lr_explr_ --learning_rate 1.2e-3
python train_hybrid_model.py 0.33x_lr_explr_ --learning_rate 1e-4

try out best lr with mixed precision:

python train_hybrid_model.py 10x_lr_explr_ --learning_rate 3e-3 --use_amp True

unfortunately mixed precision was much slower to train than the baseline, though it did reduce memory usage
the trade-off is not worth it, as if I need a larger batch size I can simply increase gradient accumulation
meanwhile the speed was the main factor in adopting it, and it came up far short

however, I did activate cudnn benchmark, and that gave a speedup of ~8%, which is nice

Now testing a change that allows the spec unet to output more channels than went into it, to allow more intermediate
channels between the spec unet and the audio unet, hopefully this will improve the performance.
It will not cost much more memory or speed hopefully, it was quite cheap parameters wise to increase
intermediate channels from 1 to 8.

python train_hybrid_model.py 8x_intermediate_channels_

unfortunately ran into some bugs, will fix later

for now, running with the best parameters found so far, and using the hybrid model
batch 64, lr 3e-3, n_train_steps 50k, gamma 0.9998 (had to adjust it to have a longer decay when training for so much longer)

python train_hybrid_model.py best_v2_
